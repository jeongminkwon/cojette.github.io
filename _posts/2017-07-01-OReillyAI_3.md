---
layout: post
title: O’Reilly AI Conference 3일차 후기
---

![image](https://cojette.files.wordpress.com/2017/06/img_6596.jpg)

드디어 대망(?)의 3일차.

# Keynotes
2, 3일자 키노트는 [유튜브](https://www.youtube.com/playlist?list=PL055Epbe6d5aWQeNSNfGwal91ruF-jsin)에 다수 올라와 있으니 필요하신 분은 참고하세요. 
키노트 답지 않게 상세한 설명을 해주는 학술적(?) 키노트가 많아서 좋았다.

## Magenta : Machine Learnin Creavity 
첫 시작은 구글의 발표. 딥 러닝으로 예술 관련 프로젝트를 하는 [마젠타](g.co/magenta) 프로젝트에 대한 설명이다. 
하지만 시작부터 음악을 들려주려고 하는데 컴퓨터에서 소리 재생이 안 되어..  ‘Audio Fail’ 이 튀어나왔다. 그러다 우예저예 고쳐지기는 했지만 나온 이후에 나온 피아노곡은 미묘하게 취향은 아닌 걸로.(…) 이 외에도 Google Quickdraw 등 여러 가지를 하고 있다고 간단히 소개하고 끝났다. 

## Cars that coordinate with people
자동 주행 관련 모델링. 주변의 운전자들을 움직이는 장애물로 볼 것인가? 협동해야 하는 대상으로 볼 것인가? 에 따라 모델링의 관점이 달라진다. 뭐든지 해당 속성을 이해하고 모델링하는 것이 중요할 것이라는 생각이 들었다. 더불어 이를 확장하면 사람과 AI의 interaction에 대해서도 고민해야 할 필요가 있을 것이다.

## Machine learning on Google Cloud Platform
Google Cloud에 대한 Sponsor keynote. NL API, Vision API도 있고 ML 관련 API 지원도 많이 되고 전무후무한 tensorflow도 있더라.

## Superhuman AI for strategic reasoning: Beating top pros in heads-up no-limit Texas hold’em
imperfect-info games에 대한 고민은 만인의 숙제. 다만 불확실성을 여러 가지 다른 관점에서 생각하고 활용할 수 있는데 이 중 확인 불명 상태를 일종의 신호로 해석해서 활용하는 것. 이런 방식으로 텍사스 홀덤 플레이 시스템(Libratus Platform)을 만든 것을 설명했다.
뭐 학습시키고 semi-supervised learning 사용하고 하는 것은 다 그냥 그런가 하지만  exploration vs exploitation vs. exploitablility를 이해하는 것이 중요하다는 관점에는 머리를 주억주억.

## Evolve AI
sponsored by Intel Nervana. 그냥 Nervana 자랑이라 별 건 없고, **다음 AI Conference부터는 O’Reilly와 Intel이 아예 컨퍼런스 공동 주최**라는 게 인상적.(…) 좋은 건가 나쁜 건가 모르겠지만 더욱 더 기업기업 해질 것은 확실한 것 같다. 

## Artificial intelligence in the software engineering workflow
피터 노빅을 실제로 영접하다니!! (동공지진). AI Software is software, and AI is like a raison in raison bread. 라는 비유를 했다. 그리고 개인 베이커리에서 이제 공장제할 정도로 빵 사업도 커지고 AI 프로그램도 여기저기서 나오지만, 모델은 프로그램이 아니고 모델 훈련하는 건 디버깅이 아니고 재훈련 하는 것이 프로그램 패치가 아니고 결과도 딱 떨어지지 않으며 무엇보다 프로그래밍과 달리 머신 러닝에는 ‘불확실성’이 다양한 형태로 존재하므로 이를 고려해야 한다는 말씀을 하셨다(그쵸 기존 다른 프로그램과 왜 다르냐며 답답해하는 SW 엔지니어는 여기도 많은가 보군요…). 그래서 AI를 하는 사람들은 기존의 micromanaging에서 벗어나서 넓은 관점으로 살펴보고 이해하는 teacher 가 되어야 할 것이라고. 


# Sessions 
## Teaching machines to reason and comprehend
https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59930

텍스트를 이해하는 모델에 대해서 관련 내용을 설명함. 시작부터 꽤나 학술적인 토크여서 머리는 멍하지만(…) 재미있었다. 

multimodal learning은 AI나 머신 러닝 좀만 하면 이제 만인이 고민하게 되는 문제 같다(사실 너무 당연한 건데).  전날의 문장 읽어주고 질의를 뽑아내고 답을 하는 제품과 유사한 것에 대한 연구였다. 

하지만 문맥의 유사도를 구하는 Gated Attention Mechanism 이 흥미로웠고 이를 사용해서 sequential reasoning을 한다는 것도 재미있었다(하지만 자세한 내용은 잘 기억이 나지 않는다. 큰일이다. 나중에 다시 찾아봐야지..)

## Bayesian Deep Learning
https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/60874

제목만 보고 일단 뛰쳐 들어갔는데 발표자가 무려 앨런 튜링 인스티튜트 소속… (동공지진)

내용은 딥러닝의 업그레이드를 위해 베이지안을 적용한 베이지안 딥러닝에 대한 설명과 그에 관련된 연구에 대한 발표였는데…우선 너무 어려웠다. 발표자료가 빡빡한데 반 이상이 수식이야… 물론 베이지안은 아무래도 확률 모델이다보니 전반적으로 식이 많을 수밖에는 없는데, 이건 좀 넘나 빡센 것…
그래도 들어오기 잘 했다고 생각하는 것은 나름 기본부터 잡아주어서?

딥러닝은 분명 좋은 것이기는 하지만-단순하지, modular하지, 사람들의 주목도 받고 있지, tensorflow처럼 도구 개발도 아주 잘 되고 있고, 성능이 좋으니 실제 영향력도 강하지- 사실 아주 단순화시켜서 말하면 성능 좋은 행렬 조합이라고 깠다(야). 물론 성능 좋으면 다 된 거 아니냐 싶기는 하지만 설명력이 없고(물론 지금이야 딥러닝이면 다들 그저 좋아 어쩔 줄 모르니 어떻게든 되지 싶지만 발목 잡히는 데가 분명 있긴 할 것이야..) 쉽게 단순화되며, 수학적 근본이 부족하고, 너무 빅데이터에 기대는 것 같다는 등의 문제점도 분명 존재한다는 것(사실 학계에 계신 분이라 너무 심하게 말하는 것 아닌가 싶지만 사실 틀린 말은 없….
무엇보다 다양한 불확실성이 존재하는데(일단 정말 black box고), 이를 해결할 수 있는 한 가지 방법이 bayesian이라고. Bayesian은 **the language of uncertainty(!)**라며 이 것이 deep learning과는 orthogonal하지만 이를 조합할 수 있는 방법도 있다며, 우선  dropout을 0, 1로 나누는 게 아니라 여기에 확률 모델을 적용하는 방법을 제시했다. (그러면서 얀 레쿤 같은 사람도 원래 bayesian neural network 하던 사람인데 요즘은 다른 데 있더라구요..이러면서 디스를.) 그리고 이를 이렇게 넣으면 기존의 uncertainty measurement metric인 information gain, entropy 등을 사용할 수 있을 것이라고.

## Anaerobic AI: Developing in a data-starved environment
https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59126

누가 인텔 강연 아니랄까봐, 시작부터 또 new oil 드립을 하는 바람에 살짝 짜증났지만(…). 제목답게 데이터를 구하기 힘들고 특히 AI의 포인트인 groundtruth labelled data는 매우 구하기도 힘들고 만드는 걸 시도하는 건 매우 infeasible한 일이라는 것을 짚고 시작하니 일단 울고 시작하며 많은 것이 용서가 되는 것이다(일단 해보지도 않고 헛소리하는 건 아니라는 건 알겠지 않는가). 

그리고 이를 어떻게 해결하나 했더니 역시나(예상한 대로) 적은 데이터를 가져다가 러닝 모델 돌리기 + 시뮬레이션해서 뻥튀기하는 것. 게다가 synthetic world라는 무시무시한 이야기를 했는데, ‘그거 privacy등 ethic 문제는 어케 해결할 건데?’ 라는 질문을 하고 싶지만 참았다. 뭐 미쿡 법은 또 제가 잘 모르니까요(먼 산). 
이 것들이 다들 결합된 내용으로, 로봇에게 학습을 시켜야 하는데 공간정보가 부족하니까, 우선 2D도면 같은 걸 구해다가 기존 open data set의 3D데이터를 학습시킨 후 그걸로 2D->3D만드는 모델을 만들어서 3D 데이터를 만든 후 그걸 가지고 로봇에게 이동 학습을 시켰다고. 하지만 이건 이거 나름대로의 스케일도 부럽고, 이런 식은 범용적으로 사용할 수도 없을 거라는 생각만 왕왕 들었다.

## Recommending products for 1.91 billion people on Facebook
https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/58142

페이스북에서 고객에게 광고 추천을 어떻게 하고 있는지에 대한 세션. 초초초 눈을 빛내며 들어갔지만 매우 제너럴한 이야기와 회사 자랑만 할 것 같았는데 의외로 설명을 많이 해서 신났다고 한다.  

각 고객의 구매 경우에 대한 값을 간략하게 어떻게 매기고 있는 지를 설명하고, 이에 대해서 기존에 흔히 사용하는 CF( collaborative filtering) + Cold Start + MF(Matrix Factorization)의 문제점을 짚은 후, 이를 Supervised Machine learning으로 푸는 법, 이 경우의 scoring을 매기는 방법 등에 대해 풀고 이를 deep learning으로 옮기는 것, 그리고 너무 많으므로 이를 각각 module화해서 나중에 합치는 방안 등에 대해 설명했다. 의외로 내용이 상세해서 즐거웠다. 

## Rules of machine learning verification: From data-driven bugs to explainable AI
https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59147

ML의 결과나 성능을 검증하는 것은 이 바닥의 너무나도 오래된 숙제 중 하나다. 디자인에서, 갑자기 생기는 외부의 이상 현상에서, 데이터가 엉망이어서… 어디서든 문제는 발생할 수 있고 이를 찾는 것은 늘 어렵고 그러다보니 데이터 분서에 대한 신뢰는 늘 널뛰기다. 그래서 이에 대해서 몇 가지 규칙을 정한 것에 대한 이야기다.

Store Data Residue (frozoen version), create adversarial examples for model,  smart reinforcement (함부로 샘플의 iid를 가정하지 말고 policy를 명확히 하자는 부분은 일단 EDA부터 제대로 하자는 말과 일맥상통한다), modular infrastructure, rationale generation (설명력과 accuracy 사전 정의) 등의 10가지 규칙이 있는데, 이 중에서 가장 인상적이었던 부분은 다음 내용이다.

**Handling ML Technical Debt.** 기술 부채 이야기는 이제 머신 러닝에서도 남의 이야기가 아니다. 그리고 이 기술 부채는 결국 **데이터**다. 데이터의 정합성, 피처 선택과 분석에 용이한 정제 정도, 접근성, 저장 유형 등. 아 정말 이 이야기 나오는데 눈물이 앞을 가리고… 데이터 관련 이야기는 매번 무시되기 일쑤지만 이제 AI와 머신 러닝의 이야기에 실려가니 데이터 기술 부채도 좀 신경써 주시죠!

## Deep Shopping Bot
https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59178

독일의 쇼핑몰 관련 머신 러닝 적용 사례. 하지만 너무 제너럴한 이야기만 해서 다소 아쉬웠다. personalization, discovery, serendipity, choice가 가장 큰 숙제라는 것을 짚을 때까지는 좋았는데…NLP 시리즈와 이미지 가지고 데이터 준비를 잘 하고 열심히 클렌징하고 파이프라인도 잘 만든 후에 super/unsupervised learning 잘 썼다 끗…이면 어찌합니까… 필터 버블을 피하기 위해서 모델을 잘 만들고 사용자들의 감정 반응을 분석하기 위한 emotional heatmap도 적용했다는데 설명은 하나도 없고!!  Docker 사용하고 데이터 처리 용으로Apache storm, spark streaming, flink, ignite 사용했다고 설명한 게 가장 자세히 설명한 것인가… 물론 건질 개념이 없던 건 아니지만 조금 아쉬운 talk 였다.

# 마무리
전보다는 depressed된 경향이 있지만, 오랜만에 넓은 곳에서 이런저런 이야기를 잔뜩 들으니 즐거웠다. 3일차에는 2일차보다 좀 더 깊이있는 이야기도 많았고, 물론 AI라고 갑자기 확 반짝반짝 떠서 다들 기합이 들어간 건 있지만 이런 내용들을 보면 갑자기 세상에 흥하는 단어가 바뀐다고 다들 이미 데이터 가지고 이런저런 것을 해오던 것 자체가 바뀐 것은 아니라는 것을 알겠다. 그리고 나도 너무 부담없이 그냥 해오던 것을 그냥 조금 더 열심히 하면 되겠다는 것도 알겠더라.
어떤 단어가 뜨고, 사람들의 괜한 기합이 많이 들어갈 수록 이런 잔치는 점점 색이 바래겠지만, 그래도 어디서든 자신의 일들을 꾸준히 하는 사람들은 있을 것이고, 그런 사람들을 만나서 많이 배웠으면 좋겠다. 

그런 기회는 계속 있을 것이고, 내가 잘 한다면 그런 기회에 나도 닿을 수 있겠지, 하고 생각한다. 이번처럼 말이다.
